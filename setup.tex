\section{Problem formulation}
\label{sec:formulation}

\newcommand{\lpce}[0]{\ensuremath{\ell_p\mbox{-CE}}}
\newcommand{\ltwoce}[0]{\ensuremath{\ell_2\mbox{-CE}}}
\newcommand{\squaredce}[0]{\ensuremath{\ell_2^2\mbox{-CE}}}
\newcommand{\topsquaredce}[0]{\ensuremath{\ell_2^2\mbox{-TCE}}}
\newcommand{\margsquaredce}[0]{\ensuremath{\ell_2^2\mbox{-MCE}}}

\subsection{Binary classification}

Let $\mathcal{X}$ be the input space and $\mathcal{Y}$ be the label space where $\mathcal{Y} = \{0, 1\}$ in the binary classification setting.
Let $X \in \mathcal{X}$ and $Y \in \mathcal{Y}$ be random variables denoting the input and label, given by an unknown joint distribution $P(X, Y)$.

Suppose we have a model $f : \mathcal{X} \to [0, 1]$ where the output of the model represents the model's confidence that the label is 1. $f$ may not be calibrated -- we define the calibration error, which examines the difference between the model's probability and the true probability given the model's output. If the calibration error is $0$ then the model is perfectly calibrated.

% \pl{in general, before introducing a formula, I always try to give the informal English description that conveys the core essence;
% here it'd be: something like 'calibration error, which examines the difference between the model's probability and true probability given the model's output'}
\begin{definition}[Calibration error]
For $p \geq 1$, the $\ell_p$ calibration error of $f : \mathcal{X} \to [0, 1]$ is given by:
\begin{align}
\lpce(f) = \Big(\expect\big[ (f(X) - \expect[Y \; | \; f(X)])^p \big] \Big)^{1/p}
\end{align}
\end{definition}
\pl{I like to use align for everything so equations be referencable}
\pl{$\ell_p\mbox{-CE}$ - is this standard? looks super awkward; can we do $\text{CE}_p$ or something? in any case, put it behind a macro}

The $\ell_2^2$ calibration error~\cite{nguyen2015posterior, hendrycks2019anomaly, kuleshov2015calibrated, hendrycks2019pretraining, murphy1973vector,degroot1983forecasters} is most commonly used but the $\ell_1$ and $\ell_{\infty}$ calibration errors are also used in the literature~\cite{guo2017calibration}.

Calibration alone is not sufficient: consider an image dataset containing $50\%$ dogs and $50\%$ cats.
If $f$ outputs $0.5$ on all inputs, $f$ is calibrated but not very useful.
We often also wish to minimize the mean-squared error -- also known as the Brier score -- as defined below.

\begin{definition}
The mean-squared error of $f : \mathcal{X} \to [0, 1]$ is given by $\mbox{MSE}(f) = \mathbb{E}[(f(X) - Y)^2]$.
\end{definition}

We often want to minimize the MSE subject to a calibration constraint. Of course, these are not orthogonal because $\mbox{MSE} = 0$ implies perfect calibration -- in fact the MSE is the sum of the $\ell_2^2$ calibration error and a `sharpness' term~\cite{murphy1973vector,degroot1983forecasters, kuleshov2015calibrated}.

\subsection{Multi-class classification}

While calibration in binary classification is well-studied,
it's less clear what to do for multi-class, where multiple definitions abound, differing in their strength. In the multi-class setting, $\mathcal{Y} = [K]$, where $[K] = \{1, \dots, K\}$ and $f : \mathcal{X} \to [0, 1]^k$ outputs a confidence measure for each class in $[K]$.

\begin{definition}[Top-label calibration]
The $\ell_2$ calibration error is given by:
\begin{align}
\topsquaredce(f) = \expect\Big[ \Big( \prob\big(Y = \argmax_{j \in [k]} M(X)_j \mid \max_{j \in [k]} M(X)_j\big) - \max_{j \in [k]} M(X)_j \Big)^2 \Big]
\end{align}
\end{definition}

We would often like the model to be calibrated on less likely predictions as well -- imagine that a medical diagnosis system says there is a $50\%$ chance a patient has a benign tumor, a $10\%$ chance she has an aggressive form of cancer, and a $40\%$ chance she has one of a long list of other conditions. We would like the model to be calibrated on all of these predictions so we define the marginal calibration error which examines, \emph{for each class}, the difference between the model's probability and the true probability of that class given the model's output.

\begin{definition}[Marginal calibration error]
\label{dfn:marginal-ce}
Let $P(j)$ denote the probability of class $j$. The $\ell_2^2$ marginal calibration error is:
\begin{align}
\margsquaredce(f) = \sum_{j = 1}^k P(j) \mathbb{E}\big[ (M(X)_j - \expect[Y = j \mid M(X)_j])^2 \big]
\end{align}
\end{definition}

Note that prior works~\cite{guo2017calibration, hendrycks2019anomaly, hendrycks2019pretraining} often claim to perform multi-class calibration but only measure top-label calibration.
% In the Appendix we discuss other notions of multi-class calibration.

\pl{is marginal calibration standard terminology? makes sense when compared with joint, but sounds a bit strange in relation to top-label}

For notational simplicity, our theory focuses on the binary classification setting. We can transform top-label calibration into a binary calibration problem -- the model outputs a probability corresponding to its top prediction, and the label represents whether the model gets it correct or not. Marginal calibration can be transformed into $K$ one-vs-all binary calibration problems where for each $k \in [K]$ the model outputs the probability associated with the $k$-th class, and the label represents whether the input belongs to the $k$-th class or not. We look at both top-label calibration and marginal calibration in our experiments.

\subsection{Recalibration}

Since most machine learning models do not output calibrated probabilities out of the box~\cite{guo2017calibration, zadrozny2001calibrated} recalibration methods take the output of an uncalibrated model, and transform it into a calibrated probability. That is, we are given a trained model $f: \mathcal{X} \to \mathcal{Z}$ and we wish to learn a recalibrator $g : \mathcal{Z} \to [0, 1]$ such that $g \circ f$ is well-calibrated.

% We say $M$ is pooled-calibrated (Kuleshov et al) if $J$ is sampled uniformly at random from $[k]$ and:
% \[ P\Big(Y = J | f_J(x) = p) \Big) \quad \forall p \]
