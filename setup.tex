\section{Setup and background}
\label{sec:formulation}

\newcommand{\lpce}[0]{\ensuremath{\ell_p\mbox{-CE}}}
\newcommand{\ltwoce}[0]{\ensuremath{\ell_2\mbox{-CE}}}
\newcommand{\squaredce}[0]{\ensuremath{\ell_2^2\mbox{-CE}}}
\newcommand{\topsquaredce}[0]{\ensuremath{\ell_2^2\mbox{-TCE}}}
\newcommand{\margsquaredce}[0]{\ensuremath{\ell_2^2\mbox{-MCE}}}

\subsection{Binary classification}

Let $\mathcal{X}$ be the input space and $\mathcal{Y}$ be the label space where $\mathcal{Y} = \{0, 1\}$ in the binary classification setting.
Let $X \in \mathcal{X}$ and $Y \in \mathcal{Y}$ be random variables denoting the input and label, given by an unknown joint distribution $P(X, Y)$. Expectations are taken over all random variables unless otherwise specified.

Suppose we have a model $f : \mathcal{X} \to [0, 1]$ where the output of the model represents the model's confidence that the label is 1. As $f$ may not be calibrated, we define the calibration error, which examines the difference between the model's probability and the true probability given the model's output. If the calibration error is $0$ then the model is perfectly calibrated.

% \pl{in general, before introducing a formula, I always try to give the informal English description that conveys the core essence;
% here it'd be: something like 'calibration error, which examines the difference between the model's probability and true probability given the model's output'}
\begin{definition}[Calibration error]
For $p \geq 1$, the $\ell_p$ calibration error of $f : \mathcal{X} \to [0, 1]$ is given by:
\begin{align}
\lpce(f) = \Big(\expect\big[ \mid f(X) - \expect[Y \; | \; f(X)] \mid^p \big] \Big)^{1/p}
\end{align}
\end{definition}

The $\ell_2^2$ calibration error~\cite{murphy1973vector,murphy1977reliability,degroot1983forecasters, nguyen2015posterior, hendrycks2019anomaly, kuleshov2015calibrated, hendrycks2019pretraining, brocker2012empirical} is most commonly used but the $\ell_1$ and $\ell_{\infty}$ calibration errors are also used in the literature~\cite{guo2017calibration, naeini2015obtaining, nixon2019calibration}.

Calibration alone is not sufficient: consider an image dataset containing $50\%$ dogs and $50\%$ cats.
If $f$ outputs $0.5$ on all inputs, $f$ is calibrated but not very useful.
We often also wish to minimize the mean-squared error---also known as the Brier score---as defined below.

\begin{definition}
The mean-squared error of $f : \mathcal{X} \to [0, 1]$ is given by $\mbox{MSE}(f) = \mathbb{E}[(f(X) - Y)^2]$.
\end{definition}

We often want to minimize the MSE subject to a calibration constraint~\cite{gneiting2005weather, gneiting2007probabilistic}. Of course, these are not orthogonal because $\mbox{MSE} = 0$ implies perfect calibration---in fact the MSE is the sum of the $\ell_2^2$ calibration error and a ``sharpness'' term~\cite{murphy1973vector,degroot1983forecasters, kuleshov2015calibrated}.

\subsection{Multiclass classification}

While calibration in binary classification is well-studied,
it's less clear what to do for multiclass, where multiple definitions abound, differing in their strength. In the multiclass setting, $\mathcal{Y} = [K]$, where $[K] = \{1, \dots, K\}$ and $f : \mathcal{X} \to [0, 1]^K$ outputs a confidence measure for each class in $[K]$.

\begin{definition}[Top-label calibration error]
The $\ell_2^2$ top-label calibration error examines the difference between the model's probability for its top prediction and the true probability of that prediction given the model's output:
\begin{align}
\topsquaredce(f) = \expect\Big[ \Big( \prob\big(Y = \argmax_{j \in [K]} f(X)_j \mid \max_{j \in [K]} f(X)_j\big) - \max_{j \in [K]} f(X)_j \Big)^2 \Big]
\end{align}
\end{definition}

We would often like the model to be calibrated on less likely predictions as well---imagine that a medical diagnosis system says there is a $50\%$ chance a patient has a benign tumor, a $10\%$ chance she has an aggressive form of cancer, and a $40\%$ chance she has one of a long list of other conditions. We would like the model to be calibrated on all of these predictions so we define the marginal calibration error which examines, \emph{for each class}, the difference between the model's probability and the true probability of that class given the model's output.

\begin{definition}[Marginal calibration error]
\label{dfn:marginal-ce}
Let $P(k)$ denote the probability of class $k$, that is, $\prob(Y = k)$. The $\ell_2^2$ marginal calibration error is:
\begin{align}
\margsquaredce(f) = \sum_{k = 1}^K P(k) \mathbb{E}\big[ (f(X)_k - \prob(Y = k \mid f(X)_k))^2 \big]
\end{align}
\end{definition}

Note that prior works~\cite{guo2017calibration, hendrycks2019anomaly, hendrycks2019pretraining} often claim to perform multiclass calibration but only measure top-label calibration---\cite{nixon2019calibration} shows that temperature scaling~\cite{guo2017calibration} scores worse than vector scaling on a marginal calibration metric, even though it has lower top-label calibration error.
% In the Appendix we discuss other notions of multiclass calibration.

\pl{is marginal calibration standard terminology? makes sense when compared with joint, but sounds a bit strange in relation to top-label}

\ak{There isn't a standard terminology here that I am aware of. Nixon et al (came out a few weeks before neurips deadline) have a similar metric called static calibration error. Seems fairly concurrent to ours, and I prefer marginal.}

For notational simplicity, our theory focuses on the binary classification setting. We can transform top-label calibration into a binary calibration problem---the model outputs a probability corresponding to its top prediction, and the label represents whether the model gets it correct or not. Marginal calibration can be transformed into $K$ one-vs-all binary calibration problems where for each $k \in [K]$ the model outputs the probability associated with the $k$-th class, and the label represents whether the input belongs to the $k$-th class or not~\cite{zadrozny2002transforming}---this is analogous to how vector scaling~\cite{guo2017calibration} extends Platt scaling. We look at both top-label calibration and marginal calibration in our experiments. There are other notions of multiclass calibration, such as joint calibration~\cite{murphy1973vector, brocker2009decomposition} and event-pooled calibration~\cite{kuleshov2015calibrated}. The former is a stronger notion of multiclass calibration that requires the entire probability \emph{vector} to be calibrated---achieving joint calibration efficiently is an important direction for future research.


\subsection{Recalibration}

Since most machine learning models do not output calibrated probabilities out of the box~\cite{guo2017calibration, zadrozny2001calibrated} recalibration methods take the output of an uncalibrated model, and transform it into a calibrated probability. That is, given a trained model $f: \mathcal{X} \to \mathcal{Z}$, let $Z = f(X)$. We are given recalibration data $T = \{ (z_i, y_i) \}_{i=1}^n$ independently sampled from $P(Z, Y)$, and we wish to learn a recalibrator $g : \mathcal{Z} \to [0, 1]$ such that $g \circ f$ is well-calibrated.

\emph{Scaling methods}, for example Platt scaling~\cite{platt1999probabilistic}, output a function $\hat{g} = \argmin_{g \in \mathcal{G}} \sum_{(z, y) \in T} \ell(g(z), y)$, where $\mathcal{G}$ is a model family, $g \in \mathcal{G}$ is continuous, and $l$ is a loss function, for example the log loss or mean-squared error. The advantage of such methods is that they converge very quickly since they only fit a single parameter.

\emph{Histogram binning} first constructs a set of bins (intervals) that partitions $[0, 1]$, formalized below.

\begin{definition}[Binning schemes]
A binning scheme $\mathcal{B}$ of size $B$ is a set of $B$ intervals $I_1, \dots, I_B$ that partitions $[0, 1]$. Given an input $z \in [0, 1]$ if $z \in I_j$ let $\beta(z) = j$ be the interval z lands in.
  \pl{rewrite: given a function $g$, define its binned version as $\hat g(z) = \text{center of $I_j$}$ for $z \in I_j$.}
% into $B$ disjoint intervals or bins, defined by bin boundaries $0 = b_0 < b_1 < ... < b_B = 1$. For $1 \leq j \leq B$, the $j$-th bin is the interval $I_j$ where $I_1 = [b_0, b_1]$ and $I_j = (b_{j-1}, b_j]$ for $j > 1$. 
\end{definition}

The bins are typically chosen such that either $I_1 = [0, \frac{1}{B}], I_2 = (\frac{1}{B}, \frac{2}{B}], \dots, I_B = (\frac{B-1}{B}, 1]$ or so that each bin contains an equal number of $z_i$ values in the recalibration data~\cite{zadrozny2001calibrated, guo2017calibration}. Histogram binning then outputs the average $y_i$ value in each bin.
