\newpage
\section{Proofs for section~\ref{sec:challenges-measuring}}
\label{sec:appendix-platt-not-calibrated}

% \begin{example}
% For any binning scheme $\mathcal{B}$ and $p \in \mathbb{Z}^+$, there exists a distribution $P$ over $X, Y$ and a function f s.t. $\lpce(f, B) = 0$ but $\lpce(f, B) = 0.5$. Note that the $\lpce$ is always between 0 and 1.
% \end{example}

% The proofs for Section~\ref{sec:challenges-measuring} unlike for the other sections, from a technical standpoint, fairly standard and not a contribution of our paper. We give them only for completeness.

\continuousNotCalibrated*

\begin{proof}
As stated in the main text, the intuition is that in each interval $I_j$ in $\bins{}$, the model could underestimate the true probability $\expect[Y \mid f(X)]$ half the time, and overestimate the probability half the time. So if we average over the entire bin the model appears to be calibrated, even though it is very uncalibrated. The proof simply formalizes this intuition.

Since $f$ is bijective and continuous we can select data distribution $P$ s.t. $f(X) \sim \mbox{Uniform}[0.5 - \epsilon, 0.5 + \epsilon]$ for any $\epsilon > 0$. To see this, first note that from real analysis since $f : [0, 1] \to [0, 1]$ and $f$ is bijective and continuous, $f^{-1}$ is also bijective and continuous.
Then we can let $X \sim f^{-1}(\mbox{Uniform}[0.5 - \epsilon, 0.5 + \epsilon])$ which has the desired property and has density.

Now, consider each interval $I_j$ in binning scheme $\bins{}$, and let $A_j = I_j \cap \mbox{Uniform}[0.5 - \epsilon, 0.5 + \epsilon]$.
If $A_j = \emptyset$ then $P(f(X) \in A_j) = 0$ so we can ignore this interval (since $f(X)$ will never land in this bin).
Let $p_j = \expect[f(X) \mid f(X) \in A_j]$.
Note that $\expect[f(X) \mid f(X) \in A_j] = \expect[f(X) \mid f(X) \in I_j]$.
Since $f(X) \in [0.5 - \epsilon, 0.5 + \epsilon]$, $p_j \in [0.5 - \epsilon, 0.5 + \epsilon]$ as well.
We will choose $P(Y)$ so that $Y$ is $1$ whenever $f(X)$ lands in the first $p_j$ fraction of interval $A_j$, and $0$ whenever $f(X)$ lands in the latter $1 - p_j$ fraction of $A_j$.
Then $\expect[Y \mid f(X) \in A_j] = p_j$, so the binned calibration error is 0.
But notice that for all $s \in [0.5 - \epsilon, 0.5 + \epsilon]$, $\expect[Y \mid f(X) = s]$ is either $0$ or $1$.
So we have:
\[ \lvert \expect[Y \mid f(X) = s] - s \rvert \geq 0.5 - \epsilon \]
That is, at every point the model is actually very miscalibrated at each $s$. By taking $\epsilon$ very small, we then get that $\lpce(p) \geq 0.5 - \epsilon'$ for any $\epsilon' > 0$, which completes the proof.
\end{proof}


\binningLowerBound*

\begin{proof}
It suffices to prove the claim for the $\ell_p^p$ error:
\[ (\lpce(f_{\bins{}}))^p \leq (\lpce(f))^p \]
This is because if $p > 0$ then $a \leq b \Leftrightarrow a^p \leq b^p$.

For $p \geq 1$, let $l(a, b) = (|a - b|)^p$.
We note that $l$ is convex in both arguments.
The proof is now a simple result of Jensen's inequality and convexity of $l$.
Suppose that $\mathcal{B}$ is given by intervals $I_1, ..., I_B$.
Let $Z = f(X)$---note that $Z$ is a random variable.

We can write $(\lpce(f_{\bins{}}))^p$ as:
\[ (\lpce(f_{\bins{}}))^p = \sum_{j=1}^B P(Z \in I_j) \; l\Big( \mathbb{E}[Z \mid Z \in I_j], \mathbb{E}[Y \mid Z \in I_j] \Big) \]
We can write $(\lpce(f))^p$ as:
\[ (\lpce(f))^p = \sum_{j=1}^B P(Z \in I_j) \; \mathbb{E}\Big[ l\big( Z, \mathbb{E}[Y \mid Z] \big) \mid Z \in I_j \Big] \]
Fix some bin $I_j \in \bins{}$. By Jensen's inequality,
\[ l\Big( \mathbb{E}[Z \mid Z \in I_j], \mathbb{E}[Y \mid Z \in I_j] \Big) \leq \mathbb{E}\Big[ l\big( Z, \mathbb{E}[Y \mid Z] \big) \mid Z \in I_j \Big] \]
Since this inequality holds for each term in the sum, it holds for the whole sum:
\[ (\lpce(f_{\bins{}}))^p \leq (\lpce(f))^p \]
Note that the proof also implies that finer binning schemes give a better lower bound.
That is, given $\bins{}'$ suppose for all $I_j' \in \bins{}'$, $I_j' \subseteq I_k$ for some $I_k \in \bins{}$.
Then $\lpce(f_{\bins{}}) \leq \lpce(f_{\bins{}'}) \leq \lpce(f)$.
This is because $f_{\bins{}'}$ can be seen as a binned version of $f_{\bins{}}$.
% Fix bin $I_j \in \bins{}$.
% Since $\bins{}'$ is finer than $\bins{}$, there exists some $S_j$ s.t. $I_j = \bigcup_{s \in S_j} I_s'$.

% Define the following notation to simplify the proof.
% \[ p_s = P(Z \in I_s' | Z \in I_j) \]
% \[ f_s = \mathbb{E}[Z | Z \in I_s']\]
% \[ y_s = \mathbb{E}[Y | Z \in I_s']\]
% Since $I_j = \bigcup_{s \in S_j} I_s'$ and the intervals are disjoint, we have:
% \[ \sum_{s \in S_j} p_s = 1 \]
% We can write $(\lpce(f, \bins{}))^p$ and $(\lpce(f_{\bins{}'}))^p$ as a weighted sum of the errors in each bin $I_j$.
% \[ (\lpce(f, \bins{}))^p = \sum_{j=1}^m P(Z \in I_j) \; l\Big( \big( \sum_{s \in S_j} p_s f_s \big), \big( \sum_{s \in S_j} p_s y_s \big) \Big) \]
% \[ (\lpce(f_{\bins{}'}))^p = \sum_{j=1}^m P(Z \in I_j) \Big( \sum_{s \in S_j} p_s l(f_s, y_s) \Big) \]
% Then by Jensen's inequality, from the convexity of $l$,
% \[ l\Big( \big( \sum_{s \in S_j} p_s f_s \big), \big( \sum_{s \in S_j} p_s y_s \big) \Big) \leq \sum_{s \in S_j} p_s l(f_s, y_s)  \]
% Since this inequality holds for each term in the sum, it holds for the whole sum:
% \[ (\lpce(f, \bins{}))^p \leq (\lpce(f_{\bins{}'}))^p \]

% The proof of the second inequality, that $(\lpce(f_{\bins{}'}))^p \leq (\lpce(f))^p$, closely parallels the first.
\end{proof}
