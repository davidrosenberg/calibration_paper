\section{Proofs for section~\ref{sec:challenges-measuring}}
\label{sec:appendix-platt-not-calibrated}

% \begin{example}
% For any binning scheme $\mathcal{B}$ and $p \in \mathbb{Z}^+$, there exists a distribution $P$ over $X, Y$ and a function f s.t. $\ell_p\mbox{-CE}(f, B) = 0$ but $\ell_p\mbox{-CE}(f, B) = 0.5$. Note that the $\ell_p\mbox{-CE}$ is always between 0 and 1.
% \end{example}

\continuousNotCalibrated*

\begin{proof}
As stated in the main text, the intuition is that in each interval $I_j$ in $\bins{}$, the model could underestimate the true probability $\expect[Y \mid f(X)]$ half the time, and overestimate the probability half the time. So if we average over the entire bin the model appears to be calibrated, even though it is very uncalibrated. The proof simply formalizes this intuition.

Since $f$ is bijective and continuous we can select data distribution $P$ s.t. $f(X) \sim \mbox{Uniform}[0.5 - \epsilon, 0.5 + \epsilon]$ for any $\epsilon > 0$. To see this, first note that from real analysis since $f : [0, 1] \to [0, 1]$ and $f$ is bijective and continuous, $f^{-1}$ is also bijective and continuous. That is, $f$ is a homeomorphism. Now, consider $I = f^{-1}([0.5 - \epsilon, 0.5 + \epsilon])$. We will choose $P(X)$ s.t. $P(X \not\in I) = 0$ and $P(X \leq f^{-1}(0.5 - \epsilon + t)) = \frac{t}{2\epsilon}$ for $0 \leq t \leq 2 \epsilon$. Since $f$ is increasing and $f^{-1}$ is continuous, this defines a valid probability distribution, and by construction $f(X) \sim \mbox{Uniform}[0.5 - \epsilon, 0.5 + \epsilon]$.

Now, consider each interval $I_j$ in binning scheme $\bins{}$. Let $p_j = \expect[f(X) \mid f(X) \in I_j]$. Since $f(X) \in [0.5 - \epsilon, 0.5 + \epsilon]$, $p_j \in [0.5 - \epsilon, 0.5 + \epsilon]$ as well. We will choose $P(Y)$ so that $Y$ is $1$ whenever $f(X)$ lands in the first $p_j$ fraction of interval $I_j$, and $0$ whenever $f(X)$ lands in the latter $1 - p_j$ fraction of $I_j$. Then $\expect[Y \mid f(X) \in I_j] = p_j$, so the binned calibration error is 0. But notice that for all $s \in [0.5 - \epsilon, 0.5 + \epsilon]$:
\[ \lvert \expect[Y \mid f(X) = s] - s \rvert \geq 0.5 - \epsilon \]
That is, at every point the model is actually very miscalibrated because $\expect[Y \mid f(X) = s]$ is either $0$ or $1$ while $s \in [0.5 - \epsilon, 0.5 + \epsilon]$. By taking $\epsilon$ very small, we then get that $\lpce(p) \geq 0.5 - \epsilon'$ for any $\epsilon' > 0$, which completes the proof.
\end{proof}


\binningLowerBound*

\begin{proof}
It suffices to prove the claim for the $\ell_p^p$ error:
\[ (\ell_p\mbox{-CE}(f, \bins{}))^p \leq (\ell_p\mbox{-CE}(f, \bins{}'))^p \leq (\ell_p\mbox{-CE}(f))^p \]
This is because if $p > 0$ then $a \leq b \Leftrightarrow a^p \leq b^p$.

For $p \geq 1$, let $l(a, b) = (|a - b|)^p$.
We note that $l$ is convex in both arguments.
The proof is now a simple result of Jensen's inequality and convexity of $l$.

We begin with the first inequality. Suppose that $\mathcal{B}$ is given by intervals $I_1, ..., I_m$. Let $Z = f(X)$. Note that $Z$ is a random variable.

% We begin with the first inequality.
% Let $Z = f(X)$, and let $J'$ be the bin in $\bins{}'$ $Z$ lands in, that is $Z \in I_{J'}'$.
% Note that $Z$ and $J'$ are random variables.
% The definition of $(\ell_p\mbox{-CE}(f, \bins{}))^p$ is:
% \[ (\ell_p\mbox{-CE}(f, \bins{}))^p = \sum_{j=1}^m P(Z \in I_j) \; l \Big( \mathbb{E}[Z | Z \in I_{J}], \mathbb{E}[Y | Z \in I_{J}] \big] \Big) \]
% Since $\bins{}'$ is finer than $\bins{}$, for every $j$ there exists some $S_j$ s.t. $I_j = \bigcup_{s \in S_j} I_s'$. Also, note that the intervals in a binning scheme are all disjoint. Therefore, we can use the law of total expectation to write $(\ell_p\mbox{-CE}(f, \bins{}))^p$ as:
% \[ (\ell_p\mbox{-CE}(f, \bins{}))^p = \sum_{j=1}^m P(Z \in I_j) \; l \Big( \mathbb{E}_{J' | Z \in I_j} \big[ \mathbb{E}[Z | Z \in I_{J'}'] \big], \mathbb{E}_{J' | Z \in I_j} \big[ \mathbb{E}[Y | Z \in I_{J'}'] \big] \Big) \]
% Where all we did was to expand the inner most expectations.
% Next, the definition of $(\ell_p\mbox{-CE}(f, \bins{}'))^p$ is:
% \[ (\ell_p\mbox{-CE}(f, \bins{}))^p = \sum_{j=1}^n P(Z \in I_j') \; l \Big( \mathbb{E}[Z | Z \in I_{J}'], \mathbb{E}[Y | Z \in I_{J}'] \big] \Big) \]
% By using the law of total expectation, we can write $(\ell_p\mbox{-CE}(f, \bins{}'))^p$ as:
% \[ (\ell_p\mbox{-CE}(f, \bins{}))^p = \sum_{j=1}^m P(Z \in I_j) \; \mathbb{E}_{J' | Z \in I_j} \Big[ l\Big( \mathbb{E}[Z | Z \in I_{J'}'], \mathbb{E}[Y | Z \in I_{J'}'] \Big) \Big] \]


Fix bin $I_j \in \bins{}$.
Since $\bins{}'$ is finer than $\bins{}$, there exists some $S_j$ s.t. $I_j = \bigcup_{s \in S_j} I_s'$.

Define the following notation to simplify the proof.
\[ p_s = P(Z \in I_s' | Z \in I_j) \]
\[ f_s = \mathbb{E}[Z | Z \in I_s']\]
\[ y_s = \mathbb{E}[Y | Z \in I_s']\]
Since $I_j = \bigcup_{s \in S_j} I_s'$ and the intervals are disjoint, we have:
\[ \sum_{s \in S_j} p_s = 1 \]
We can write $(\ell_p\mbox{-CE}(f, \bins{}))^p$ and $(\ell_p\mbox{-CE}(f, \bins{}'))^p$ as a weighted sum of the errors in each bin $I_j$.
\[ (\ell_p\mbox{-CE}(f, \bins{}))^p = \sum_{j=1}^m P(Z \in I_j) \; l\Big( \big( \sum_{s \in S_j} p_s f_s \big), \big( \sum_{s \in S_j} p_s y_s \big) \Big) \]
\[ (\ell_p\mbox{-CE}(f, \bins{}'))^p = \sum_{j=1}^m P(Z \in I_j) \Big( \sum_{s \in S_j} p_s l(f_s, y_s) \Big) \]
Then by Jensen's inequality, from the convexity of $l$,
\[ l\Big( \big( \sum_{s \in S_j} p_s f_s \big), \big( \sum_{s \in S_j} p_s y_s \big) \Big) \leq \sum_{s \in S_j} p_s l(f_s, y_s)  \]
Since this inequality holds for each term in the sum, it holds for the whole sum:
\[ (\ell_p\mbox{-CE}(f, \bins{}))^p \leq (\ell_p\mbox{-CE}(f, \bins{}'))^p \]

The proof of the second inequality, that $(\ell_p\mbox{-CE}(f, \bins{}'))^p \leq (\ell_p\mbox{-CE}(f))^p$, closely parallels the first.

Fix some bin $I_j' \in \bins{}'$. We can write $(\ell_p\mbox{-CE}(f, \bins{}'))^p$ as:
\[ (\ell_p\mbox{-CE}(f, \bins{}'))^p = \sum_{j=1}^n P(Z \in I_j') \; l\Big( \mathbb{E}[Z | Z \in I_j'], \mathbb{E}[Y | Z \in I_j'] \Big) \]
We can write $(\ell_p\mbox{-CE}(f))^p$ as:
\[ (\ell_p\mbox{-CE}(f))^p = \sum_{j=1}^n P(Z \in I_j) \; \mathbb{E}\Big[ l\big( Z, \mathbb{E}[Y | Z] \big) \; | \; Z \in I_j' \Big] \]
By Jensen's inequality,
\[ l\Big( \mathbb{E}[Z | Z \in I_j'], \mathbb{E}[Y | Z \in I_j'] \Big) \leq \mathbb{E}\Big[ l\big( Z, \mathbb{E}[Y | Z] \big) \; | \; Z \in I_j' \Big] \]
Since this inequality holds for each term in the sum, it holds for the whole sum:
\[ (\ell_p\mbox{-CE}(f, \bins{}'))^p \leq (\ell_p\mbox{-CE}(f))^p \]
\end{proof}
