\section{Additional related work}

Calibration, including the $\ell_2^2$ calibration error, has been studied in many fields such as meteorology~\cite{murphy1973vector, murphy1977reliability, degroot1983forecasters,gneiting2005weather, brocker2009decomposition}, medicine~\cite{jiang2012calibrating, crowson2017calibration, harrell1996prognostic}, reinforcement learning~\cite{malik2019calibrated}, natural language processing~\cite{nguyen2015posterior, card2018calibration}, speech recognition~\cite{dong2011calibration}, econometrics~\cite{gneiting2007probabilistic}, psychology~\cite{lichtenstein1982calibration}, and machine learning~\cite{guo2017calibration, zadrozny2001calibrated, kuleshov2018accurate, zadrozny2002transforming, naeini2014binary, hendrycks2019anomaly, hendrycks2019pretraining}. Besides the calibration error metric, prior work also uses the Hosmer-Lemeshov test~\cite{hosmer1980goodness} and reliability diagrams~\cite{degroot1983forecasters, brocker2007reliability} to evaluate calibration. Besides calibration, other ways of producing and quantifying uncertainties include Bayesian methods~\cite{gelman1995bayesian} and conformal prediction~\cite{shafer2008tutorial, lei2016distribution}.

Recalibration is related to (conditional) density estimation~\cite{wasserman2019, parzen1962} as the goal is to estimate $\expect[Y \mid f(X)]$. The $L_2$ loss function used in the density estimation literature is precisely the $\ell_2^2$ calibration error. Algorithms and analysis in density estimation typically assume the true density is $L-$Lipschitz, while in calibration applications, the calibration error of the final model should be measurable from data, without making untestable assumptions on $L$---this leads to different techniques and analysis.
% whereas in calibration applications we want to bound our calibration error without making unverifiable assumptions on $L$


Bias is a common issue with statistical estimators, for example for the sample standard deviation. It has also long been known that the mean-squared error, measured on samples, gives a biased estimate---the seminal work by Stein~\cite{stein81sure} investigates and fixes this bias. However, debiasing an estimator does not typically lead to \emph{an improved sample complexity}, as it does in our case.

\pl{first impression is that this looks really brief...you can say calibration has been studied traditionally in many fields: meteorology (cite), ML (cite), etc.;
then is there's probably non calibration work that's related...just non-parametric function estimation...how is calibration different?
anything related for proof techniques?
}

\section{Conclusion}

In this paper we had three contributions: 1. We showed that the calibration error of popular continuous methods is underestimated; 2. We introduced the first method, to our knowledge, that has better sample complexity than histogram binning but has a \emph{measurable calibration error}, giving us the best of both worlds of scaling and binning; and 3. We showed that an alternative estimator has better sample complexity than the commonly used plugin estimator. Our method gives us 35\% lower calibration error on CIFAR-10, and up to 5x lower calibration error on ImageNet, with $B = 100$ bins.

\section{Acknowledgements}

The authors would like to thank the Open Philantropy Project, Stanford Graduate Fellowship, and Toyota Research Institute for funding. Toyota Research Institute (``TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.

We are grateful to Pang Wei Koh, Chuan Guo, Anand Avati, Shengjia Zhao, Weihua Hu, Yu Bai, John Duchi, Dan Hendrycks, Jonathan Uesato, Michael Xie, Albert Gu, Aditi Raghunathan, Fereshte Khani, Stefano Ermon, Eric Nalisnick, and Pushmeet Kohli for insightful discussions. We thank the anonymous reviewers for their thorough reviews and suggestions that have improved our paper. We would also like to thank Pang Wei Koh, Yair Carmon, Albert Gu, Rachel Holladay, and Michael Xie for their inputs on our draft, and Chuan Guo for providing code snippets from their temperature scaling paper.

\newpage
% We hope future work looks at even stronger notions of multiclass calibration (prior work typically focuses on top-label calibration) and calibration under domain shift.

% We think our framework opens up many new avenues for exploration. Can we come up with a binning scheme that is better than the well-balanced binning scheme that is one that leads estimate the calibration error even faster, at least for 

% \tm{I guess we need a conclusion section. }

% \tm{briefly summarize the contribution (now we can use slightly different language because we assume the readers have read most of the paper and now the definitions.)}
% \tm{Open question or future directions, potential implication of the paper}

