\section{Related work}

Calibration has been studied and used in many fields including meteorology~\cite{murphy1973vector, murphy1977reliability, degroot1983forecasters,gneiting2005weather, brocker2009decomposition}, medicine~\cite{jiang2012calibrating, crowson2017calibration, harrell1996prognostic}, reinforcement learning~\cite{malik2019calibrated}, natural language processing~\cite{nguyen2015posterior, card2018calibration}, speech recognition~\cite{dong2011calibration}, econometrics~\cite{gneiting2007probabilistic}, and psychology~\cite{lichtenstein1982calibration}. Our work is inspired in part by recent methods for calibration~\cite{guo2017calibration, kuleshov2018accurate, hendrycks2019anomaly}. In these papers, binning is used to evaluate the calibration error of models, whereas in our case it is part of the method itself, which is key to the guarantees we give. Besides the calibration error metric, prior work also uses the Hosmer-Lemeshov test~\cite{hosmer1980goodness} and reliability diagrams~\cite{degroot1983forecasters, brocker2007reliability} to evaluate calibration. Besides calibration, there are many other ways of producing and quantifying uncertainties, including Bayesian methods~\cite{gelman1995bayesian} and conformal prediction~\cite{shafer2008tutorial, lei2016distribution}.

Bias is a common issue with statistical estimators, for example for the sample standard deviation. It has also long been known that the mean-squared error, measured on samples, gives a biased estimate---the seminal work by Stein~\cite{stein81sure} investigates and fixes this bias. However, debiasing an estimator does not typically lead to \emph{an improved sample complexity.}

\pl{first impression is that this looks really brief...you can say calibration has been studied traditionally in many fields: meteorology (cite), ML (cite), etc.;
then is there's probably non calibration work that's related...just non-parametric function estimation...how is calibration different?
anything related for proof techniques?
}

\section{Conclusion}

In this paper we had three contributions: 1. We showed that the calibration error of popular continuous models is underestimated; 2. We introduced the first method, to our knowledge, that has better sample complexity than histogram binning but has a \emph{measurable calibration error}, giving us the best of both worlds of scaling and binning; and 3. We showed that an alternative estimator has better sample complexity than the commonly used plugin estimator. Our method gives us 35\% lower calibration error on CIFAR-10, and up to 5x lower calibration error on ImageNet, when we use $B = 100$ bins.

% We hope future work looks at even stronger notions of multiclass calibration (prior work typically focuses on top-label calibration) and calibration under domain shift.

% We think our framework opens up many new avenues for exploration. Can we come up with a binning scheme that is better than the well-balanced binning scheme that is one that leads estimate the calibration error even faster, at least for 

% \tm{I guess we need a conclusion section. }

% \tm{briefly summarize the contribution (now we can use slightly different language because we assume the readers have read most of the paper and now the definitions.)}
% \tm{Open question or future directions, potential implication of the paper}

