\section{Related work}

Calibration (also known as reliability) including the $\ell_2^2$ calibration error and its connection to the Brier score has been explored extensively in the statistics and meteorology literature~\cite{murphy1973vector, murphy1977reliability, degroot1983forecasters,gneiting2005weather, brocker2009decomposition}. The importance of calibration has also been emphasized in medicine~\cite{jiang2012calibrating, crowson2017calibration, harrell1996prognostic}, reinforcement learning~\cite{malik2019calibrated}, natural language processing~\cite{nguyen2015posterior, card2018calibration}, speech recognition~\cite{dong2011calibration}, econometrics~\cite{gneiting2007probabilistic}, and psychology~\cite{lichtenstein1982calibration}. Our work is inspired in part by recent work on calibration~\cite{guo2017calibration, kuleshov2018accurate, hendrycks2019anomaly}. In these papers, binning is used to evaluate the calibration error of models, whereas in our case it is part of the method itself, which is key to the guarantees we give. Besides the calibration error metric, prior work also uses the Hosmer-Lemeshov test~\cite{hosmer1980goodness} and reliability diagrams~\cite{degroot1983forecasters, brocker2007reliability} to evaluate calibration.

\pl{first impression is that this looks really brief...you can say calibration has been studied traditionally in many fields: meteorology (cite), ML (cite), etc.;
then is there's probably non calibration work that's related...just non-parametric function estimation...how is calibration different?
anything related for proof techniques?
}

\section{Conclusion}

In this paper we had three main contributions: 1. We showed that the calibration error of popular continuous models is typically underestimated, 2. We introduced the first method, to our knowledge, that has better sample complexity than histogram binning but has a \emph{measurable \pl{I think you said verifiable before, I but I actually like 'measurable'} calibration error}, and 3. We showed that an alternative estimator has better sample complexity than the commonly used plugin estimator. We showed experimental results\pl{vague - what is the actual result?} on top-label and marginal calibration for CIFAR-10 and ImageNet.

\pl{there's 4 contributions here; can improve the writing here - it's kind of a worse version of the abstract right now;
I'd use the opportunity to frame it as getting the best of both worlds of scaling and binning
}
% We hope future work looks at even stronger notions of multi-class calibration (prior work typically focuses on top-label calibration) and calibration under domain shift.

% We think our framework opens up many new avenues for exploration. Can we come up with a binning scheme that is better than the well-balanced binning scheme that is one that leads estimate the calibration error even faster, at least for 

% \tm{I guess we need a conclusion section. }

% \tm{briefly summarize the contribution (now we can use slightly different language because we assume the readers have read most of the paper and now the definitions.)}
% \tm{Open question or future directions, potential implication of the paper}

