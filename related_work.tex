\section{Additional related work}

Calibration, including the $\lsquared$ calibration error, has been studied in many fields such as meteorology~\cite{murphy1973vector, murphy1977reliability, degroot1983forecasters,gneiting2005weather, brocker2009decomposition}, fairness~\cite{johnson2018multicalibration, liu2019implicit}, healthcare~\cite{jiang2012calibrating, crowson2017calibration, harrell1996prognostic, yadlowsky2019calibration}, reinforcement learning~\cite{malik2019calibrated}, natural language processing~\cite{nguyen2015posterior, card2018calibration}, speech recognition~\cite{dong2011calibration}, econometrics~\cite{gneiting2007probabilistic}, psychology~\cite{lichtenstein1982calibration}, and machine learning~\cite{guo2017calibration, zadrozny2001calibrated, kuleshov2018accurate, zadrozny2002transforming, naeini2014binary, hendrycks2019anomaly, hendrycks2019pretraining, kull2019temperature}.
Prior work also uses the Hosmer-Lemeshov test~\cite{hosmer1980goodness} and reliability diagrams~\cite{degroot1983forecasters, brocker2007reliability} to evaluate calibration.
Besides calibration, other ways of producing and quantifying uncertainties include Bayesian methods~\cite{gelman1995bayesian} and conformal prediction~\cite{shafer2008tutorial, lei2016distribution}.

Recalibration is related to (conditional) density estimation~\cite{wasserman2019, parzen1962} as the goal is to estimate $\expect[Y \mid f(X)]$.
% The $L_2$ loss function used in the density estimation literature is closely related to the $\lsquared$ calibration error.
Algorithms and analysis in density estimation typically assume the true density is $L-$Lipschitz, while in calibration applications, the calibration error of the final model should be measurable from data, without making untestable assumptions on $L$.


Bias is a common issue with statistical estimators, for example, the seminal work by Stein~\cite{stein81sure} fixes the bias of the mean-squared error. However, debiasing an estimator does not typically lead to \emph{an improved sample complexity}, as it does in our case.

\section{Conclusion and future work}

In this paper we had three contributions: 1. We showed that the calibration error of continuous methods is underestimated; 2. We introduced the first method, to our knowledge, that has better sample complexity than histogram binning but has a \emph{measurable calibration error}, giving us the best of scaling and binning methods; and 3. We showed that an alternative estimator has better sample complexity than the plugin estimator.
% Our method gives us 35\% lower calibration error on CIFAR-10, and up to 5x lower calibration error on ImageNet, with $B = 100$ bins.
% There are many exciting avenues for future work:
% \begin{enumerate}
% \item \textbf{Dataset shifts}: Can we maintain calibration under dataset shifts? When the dataset shifts (for example, train on MNIST, but evaluate on SVHN) it is difficult to get very high accuracies on the target dataset, but can we at least know when our model is accurate and when it is not, or in other words can we ensure our model is calibrated on the new dataset? When a small amount of labeled \emph{target} data is available, we can simply re-calibrate using the labeled target data since recalibration simply involves tuning a small number of parameters and therefore requires very few samples. However, can we maintain calibration when we do not have labeled examples from the target dataset? 
% \item \textbf{Measuring calibration}: Can we come up with alternative metrics that still capture a notion of calibration, but are measurable for scaling methods?
% % Many papers use calibration error metrics such as the $\lsquared$, $\ell_2$, $\ell_1$ calibration error.
% \item \textbf{Multiclass calibration}: Most papers on calibration focus on top-label calibration---can we come up with better methods for marginal calibration? Can we achieve stronger notions of calibration, such as joint calibration, efficiently? 
% \item \textbf{Better binning}: Can we devise better ways of binning? While binning makes the calibration error measurable, it leads to an increase in the mean-squared error. In Proposition~\ref{prop:mse-finite-binning} we bound the increase in MSE for the well-balanced binning scheme. However, in Section~\ref{sec:alt_binning_schemes} we give intuition for why other binning schemes may have a smaller increase in MSE.
% \item \textbf{Finite sample guarantees}: Can we prove finite sample guarantees for Theorem~\ref{thm:final-calib} which show the dependency on the dimension and probability of failure?
% % We believe this is technically fairly challenging but interesting, because standard finite sample error bounds scale as $\frac{1}{\sqrt{n}}$ instead of $\frac{1}{n}$. However, can we leverage some properties of the calibration family to get better rates?
% \item \textbf{Estimating calibration}: The plugin estimator for the $\ell_1$ calibration error (called the average calibration error in~\cite{guo2017calibration, nixon2019calibration}) is also biased---can we come up with a more sample efficient estimator for the $\ell_1$ calibration error? For the $\lsquared$ calibration error, can we devise an even better estimator? If not, can we prove minimax lower bounds on this?
% \end{enumerate}
There are many exciting avenues for future work:
\begin{enumerate}
\item \textbf{Dataset shifts}: Can we maintain calibration under dataset shifts (for example, train on MNIST, but evaluate on SVHN)? When a small amount of labeled \emph{target} data is available, we can simply re-calibrate using the labeled target data since recalibration requires very few samples, but can we maintain calibration without labeled examples from the target dataset?
\item \textbf{Measuring calibration}: Can we come up with alternative metrics that still capture a notion of calibration, but are measurable for scaling methods?
\item \textbf{Estimating calibration}: The plugin estimator for the $\ell_1$ calibration error (called the empirical calibration error in~\cite{guo2017calibration, nixon2019calibration}) is also biased---can we come up with a more sample efficient estimator for the $\ell_1$ calibration error? Our preliminary experiments suggest that the Bootstrap~\cite{efron1979bootstrap} could be a generic way to debias the $\ell_p$ calibration error, can we show this?
% Many papers use calibration error metrics such as the $\lsquared$, $\ell_2$, $\ell_1$ calibration error.
\item \textbf{Multiclass calibration}: Can we come up with better methods for marginal calibration? Can we achieve stronger notions of calibration, such as joint calibration, efficiently? 
% \item \textbf{Better binning}: Can we devise better ways of binning? While binning makes the calibration error measurable, it increases the mean-squared error. In Section~\ref{sec:alt_binning_schemes} we give intuition for why alternative binning schemes may have a smaller increase in MSE.
% \item \textbf{Finite sample guarantees}: Can we prove finite sample guarantees for Theorem~\ref{thm:final-calib} which show the dependency on the dimension and probability of failure?
% We believe this is technically fairly challenging but interesting, because standard finite sample error bounds scale as $\frac{1}{\sqrt{n}}$ instead of $\frac{1}{n}$. However, can we leverage some properties of the calibration family to get better rates?

\end{enumerate}

\section{Acknowledgements}

The authors would like to thank the Open Philantropy Project, Stanford Graduate Fellowship, and Toyota Research Institute for funding. Toyota Research Institute (``TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.

We are grateful to Pang Wei Koh, Chuan Guo, Anand Avati, Shengjia Zhao, Weihua Hu, Yu Bai, John Duchi, Dan Hendrycks, Jonathan Uesato, Michael Xie, Albert Gu, Aditi Raghunathan, Fereshte Khani, Stefano Ermon, Eric Nalisnick, and Pushmeet Kohli for insightful discussions. We thank the anonymous reviewers for their thorough reviews and suggestions that have improved our paper. We would also like to thank Pang Wei Koh, Yair Carmon, Albert Gu, Rachel Holladay, and Michael Xie for their inputs on our draft, and Chuan Guo for providing code snippets from their temperature scaling paper.

\newpage

% We hope future work looks at even stronger notions of multiclass calibration (prior work typically focuses on top-label calibration) and calibration under domain shift.

% We think our framework opens up many new avenues for exploration. Can we come up with a binning scheme that is better than the well-balanced binning scheme that is one that leads estimate the calibration error even faster, at least for 

% \tm{I guess we need a conclusion section. }

% \tm{briefly summarize the contribution (now we can use slightly different language because we assume the readers have read most of the paper and now the definitions.)}
% \tm{Open question or future directions, potential implication of the paper}

