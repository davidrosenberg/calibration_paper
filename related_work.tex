\section{Additional related work}

Calibration, including the $\lsquared$ calibration error, has been studied in many fields such as meteorology~\cite{murphy1973vector, murphy1977reliability, degroot1983forecasters,gneiting2005weather, brocker2009decomposition}, medicine~\cite{jiang2012calibrating, crowson2017calibration, harrell1996prognostic}, reinforcement learning~\cite{malik2019calibrated}, natural language processing~\cite{nguyen2015posterior, card2018calibration}, speech recognition~\cite{dong2011calibration}, econometrics~\cite{gneiting2007probabilistic}, psychology~\cite{lichtenstein1982calibration}, and machine learning~\cite{guo2017calibration, zadrozny2001calibrated, kuleshov2018accurate, zadrozny2002transforming, naeini2014binary, hendrycks2019anomaly, hendrycks2019pretraining}. Besides the calibration error metric, prior work also uses the Hosmer-Lemeshov test~\cite{hosmer1980goodness} and reliability diagrams~\cite{degroot1983forecasters, brocker2007reliability} to evaluate calibration. Besides calibration, other ways of producing and quantifying uncertainties include Bayesian methods~\cite{gelman1995bayesian} and conformal prediction~\cite{shafer2008tutorial, lei2016distribution}.

Recalibration is related to (conditional) density estimation~\cite{wasserman2019, parzen1962} as the goal is to estimate $\expect[Y \mid f(X)]$. The $L_2$ loss function used in the density estimation literature is closely related to the $\lsquared$ calibration error. Algorithms and analysis in density estimation typically assume the true density is $L-$Lipschitz, while in calibration applications, the calibration error of the final model should be measurable from data, without making untestable assumptions on $L$---this leads to different techniques and analysis.


Bias is a common issue with statistical estimators, for example for the sample standard deviation. It has also long been known that the mean-squared error, measured on samples, gives a biased estimate---the seminal work by Stein~\cite{stein81sure} investigates and fixes this bias. However, debiasing an estimator does not typically lead to \emph{an improved sample complexity}, as it does in our case.

\section{Conclusion and future work}

In this paper we had three contributions: 1. We showed that the calibration error of popular continuous methods is underestimated; 2. We introduced the first method, to our knowledge, that has better sample complexity than histogram binning but has a \emph{measurable calibration error}, giving us the best of both worlds of scaling and binning; and 3. We showed that an alternative estimator has better sample complexity than the commonly used plugin estimator. Our method gives us 35\% lower calibration error on CIFAR-10, and up to 5x lower calibration error on ImageNet, with $B = 100$ bins.

We believe there are many exciting avenues for future work:
\begin{enumerate}
\item \textbf{Measuring calibration}: Many papers use calibration error metrics such as the $\lsquared$, $\ell_2$, $\ell_1$ calibration error. Can we come up with alternative metrics that still capture a notion of calibration, but are measurable for scaling/continuous methods?
\item \textbf{Multiclass calibration}: How can we achieve stronger notions of calibration, such as joint calibration, efficiently? Many papers on calibration focus on top-label calibration---a more systematic study on how calibration methods perform for multiclass calibration metrics like the marginal calibration error, could be very useful.
\item \textbf{Better binning}: Can we devise better ways of binning? While binning makes the calibration error measurable, it leads to an increase in the mean-squared error. In Proposition~\ref{prop:mse-finite-binning} we bound the increase in MSE for the well-balanced binning scheme. However, in Section~\ref{sec:alt_binning_schemes} we give intuition for why other binning schemes may have a smaller increase in MSE---we think this could be an interesting direction to pursue. A different way of binning may also give us a better sample complexity for measuring the calibration error.
\item \textbf{Finite sample guarantees}: We give asymptotic rates for the variance-reduced calibrator. Can we prove finite sample guarantees which show the dependency on the dimension and probability of failure? We believe this is technically fairly challenging but interesting, because standard finite sample error bounds scale as $\frac{1}{\sqrt{n}}$ instead of $\frac{1}{n}$. However, can we leverage some properties of the calibration family to get better rates?
\item \textbf{Estimating calibration}: Can we devise an even better estimator to measure the calibration error? If not, can we prove minimax lower bounds on this?
\end{enumerate}

\section{Acknowledgements}

The authors would like to thank the Open Philantropy Project, Stanford Graduate Fellowship, and Toyota Research Institute for funding. Toyota Research Institute (``TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.

We are grateful to Pang Wei Koh, Chuan Guo, Anand Avati, Shengjia Zhao, Weihua Hu, Yu Bai, John Duchi, Dan Hendrycks, Jonathan Uesato, Michael Xie, Albert Gu, Aditi Raghunathan, Fereshte Khani, Stefano Ermon, Eric Nalisnick, and Pushmeet Kohli for insightful discussions. We thank the anonymous reviewers for their thorough reviews and suggestions that have improved our paper. We would also like to thank Pang Wei Koh, Yair Carmon, Albert Gu, Rachel Holladay, and Michael Xie for their inputs on our draft, and Chuan Guo for providing code snippets from their temperature scaling paper.

\newpage
% We hope future work looks at even stronger notions of multiclass calibration (prior work typically focuses on top-label calibration) and calibration under domain shift.

% We think our framework opens up many new avenues for exploration. Can we come up with a binning scheme that is better than the well-balanced binning scheme that is one that leads estimate the calibration error even faster, at least for 

% \tm{I guess we need a conclusion section. }

% \tm{briefly summarize the contribution (now we can use slightly different language because we assume the readers have read most of the paper and now the definitions.)}
% \tm{Open question or future directions, potential implication of the paper}

