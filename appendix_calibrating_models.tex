\newpage
\section{Proofs for section~\ref{sec:calibrating_models}}
\label{sec:calibrating_models_appendix}

\newcommand{\G}[0]{\ensuremath{\mathcal{G}}}

Our analysis of the sample complexity of \ourcal{} requires some assumptions on the function family $\G{}$ that hold for standard methods like Platt scaling, beta calibration, and vector scaling (since vector scaling calibrates each class separately):

\begin{enumerate}
\item (Finite bounded parameters). Let $\G{} = \{ g_{\theta} : \mathcal{Z} \to [0, 1] \; | \; \theta \in \mathbb{R}^{d'} \wedge ||\theta||_{\infty} \leq R \}$
\item (Injective). For all $g_{\theta} \in \G{}$ we assume $g_{\theta}$ is injective.
\item (Lipschitz). Suppose that for all $z \in \mbox{supp}(Z)$, $|g_{\theta_1}(z) - g_{\theta_2}(z)| \leq L|\theta_1 - \theta_2|_2$.
\end{enumerate}

If $\G{}$ satisfies the following assumptions we say $\G{}$ is $(d', L, R)$-compatible. For Platt-scaling $d' = 1$ and $L$ and $R$ are typically small constants.

% We first need to introduce the Bayes optimal recalibrator $\omega^*$, which we compare all other recalibrators to. $\omega^* \circ f$ has calibration error $0$, and also has the minimum mean-squared error among all recalibrators.

% \begin{definition}
% $\omega^* : \mathcal{Z} \to [0, 1]$ is given by $\omega^*(x) = \expect[Y | Z = z]$.
% \end{definition}

\subsection{Calibration bound (Proof of Theorem~\ref{thm:final-calib})}

The goal is to prove the following theorem from Section~\ref{sec:calibrating_models}, which we restate:

\finalCalib*{}


We will analyze each step of our algorithm and then combine the pieces to get Theorem~\ref{thm:final-calib}.
As we mention in the main text, step 3 is the main variance-reduction step, so Lemma~\ref{thm:empirical-binning} is one of the core parts of our proof.
Step 2 is where we construct a binning scheme so that each bin has an equal number of points---we show that this property holds approximately in the population.
This is important as well, particularly to ensure we can estimate the calibration error.
Step 1 is a simple adaptation of a standard result from learning theory.

We first show a generalization result for Step 1 of our approach---recall that step 1 is essentially Platt scaling.
That is, if $\G{}$ is $(d', L, R)$-compatible and contains $g^* \in \G{}$ with low calibration error, then we show that the empirical risk minimizer $g \in \G{}$ of the mean-squared loss will also quickly converge to low calibration error.
Intuitively, methods like Platt scaling fit a single parameter to the data so classical learning theory bounds tell us they will converge quickly to their optimal error, at least in mean-squared error.
We can combine this with a decomposition of the mean-squared error into calibration and refinement, and the injectivity of $g \in \mathcal{G}$, to show they also converge in calibration error.

\begin{lemma}[Convergence of Platt scaling]
\label{lem:platt_scaling_bound}
There exists a constant $c$, independent of $d', L, n$ such that with high probability over the recalibration samples $\squaredce(g) \leq \min_{g' \in \G{}}\squaredce(g') + \frac{cLd' \log{R}}{\sqrt{n}}$, where recall that $g \in \G{}$ was selected as the empirical risk minimizer of the mean-squared error loss.
\end{lemma}

\begin{proof}
We use the classic decomposition of the mean-squared error into calibration error (also known as reliability) and refinement\footnote{Note that the refinement term can be further decomposed into resolution (also known as sharpness) and irreducible uncertainty.}. For any $g \in \G{}$ we have:
\[ \mbox{MSE}(g) = \underbrace{\squaredce(g)}_{\mbox{calibration}} + \underbrace{\expect[(\expect[Y \mid g(Z)] - Y)^2]}_{\mbox{refinement}} \]
Note that the refinement term is constant for all injective $g \in \G{}$, since for injective $g$:
\[ \expect[(\expect[Y \mid g(Z)] - Y)^2] = \expect[(\expect[Y \mid Z] - Y)^2] \]
This means that the difference in $\mbox{MSE}$ between any $g$ and $g'$ is precisely the difference in the mean-squared error. From a standard $\epsilon$-cover in the parameter space, or PAC-Bayes argument, we can get a high probability generalization bound on the mean-squared error:
\[ \mbox{MSE}(g) \leq \mbox{MSE}(g^*) + \frac{cLd' \log{R}}{\sqrt{n}} \]
This gives us the desired result.
\end{proof}

Our next lemma will require showing convergence in $\ell_2$ and $\ell_1$ norm in function space, which we define below:

\begin{definition}[Distances between functions]
Given $f, g : \mathcal{Z} \to [0, 1]$, for the $\ell_2$ norm we define $||f - g||_2^2 = \expect[(f(Z) - g(Z))^2]$ and $||f- g||_2 = \sqrt{||f - g||_2^2}$. For the $\ell_1$ norm we define $||f - g||_1 = \expect[\lvert f(Z) - g(Z)\rvert]$
\end{definition}

Recall that we showed that in the limit of infinite data the binned version of $g$, $g_{\bins{}}$, has lower calibration error than $g$ (Proposition~\ref{prop:bin_low_bound}). However our method uses $n$ data points to empirically bin $g$, giving us $\hat{g_{\bins{}}}$. We now show the key lemma that allows us to bound the calibration error and later the mean-squared error. That is, we show that the empirically binned function $\hat{g_{\bins{}}}$ quickly converges to $g_{\bins{}}$ in both $\ell_2$ and $\ell_1$ norms.

\begin{lemma}[Empirical binning]
\label{thm:empirical-binning}
There exist constants $c_B, c_1, c_2$ such that the following is true. Given $g : \mathcal{Z} \to [0, 1]$, binning set $T_3 = \{(z_i, y_i)\}_{i=1}^n$ and a 2-well-balanced binning scheme $\bins{}$ of size $B$. Given $0 < \delta < 0.5$, suppose that $n \geq c_B B\log{\frac{B}{\delta}}$. Then with probability at least $1 - \delta$,  $||\hat{g_{\bins{}}} - g_{\bins{}}||_2 \leq \frac{c_2}{\sqrt{n}}\sqrt{\log{\frac{B}{\delta}}}$ and $||\hat{g_{\bins{}}} - g_{\bins{}}||_1 \leq \frac{c_1}{\sqrt{nB}}\sqrt{\log{\frac{B}{\delta}}}$
\end{lemma}

\begin{proof}
% Note, for the L2 bound we only need 1 side of 2-well-balanced, P(f(X) \in I_j) \geq 1/2B for all j.
% For the L1 bound, if we have only 1 side of 2-well-balanced, but b_j - b_{j-1} are equal for all j, then we get the same bound.
% One question is whether we can relax this theorem, for example if b_j - b_{j-1} are equal for all j. Maybe we don't need well-balancedness in that case.
% We first discuss high level intuition for the proof. Recall that in the naive binning approach, where we bin the label (that is, $Y$) values, the $\ell_2$ calibration error is $O(\sqrt{\frac{B}{n}})$ ignoring $\log$ factors. That is, the number of samples we need for calibration increases linearly with the number of bins. The intuition is that when we have more bins, we have fewer samples per bin. That is, we have $\frac{n}{B}$ samples of $Y$ in each bin, where $Y \in \{0, 1\}$. Then Hoeffding's bound gives us the above result, which is tight up to constants. However, this theorem shows when we bin $f$ instead of $Y$, our rates have much better dependencies on $B$. The high level idea is that for each bin $j$ we are taking the average of values bounded in $I_j = [b_{j-1}, b_j]$. This is a narrower range than in naive binning where we took the average of values in $\{0, 1\}$. Since the values are bounded in a narrower range, the variance is lower. Of course, the variance of any particular bin could be big. For example we could have $I_7 = [0.1, 0.9]$. But the sum of all the bin sizes is 1, so `most' of the bins have a lower variance. We now give the formal proof.

Recall that the intuition is in Figure~\ref{fig:variance_reduced_illustration} of the main text---the $g(z_i)$ values in each bin (gray circles in Figure~\ref{fig:var_red_binning}) are in a narrower range than the $y_i$ values (black crosses in Figure~\ref{fig:hist_binning}) so when we take the average we incur less of an estimation error. Now, there may be a small number of bins where the $g(z_i)$ values are not in a narrow range, but we will use the assumption that $\bins{}$ is 2-well-balanced to show that these effects average out and the overall estimation error is small.

Define $R_j$ to be the set of $g(z_i)$ that fall into the $j$-th bin, given by $R_j = \{g(z_i) \mid g(z_i) \in I_j \wedge (z_i, y_i) \in T_3\}$ (recall that $T_3$ is the data we use in step 3).
Let $p_j$ be the probability of landing in bin $j$, given by $p_j = \prob(g(Z) \in I_j)$.
Since $\bins{}$ is 2-well-balanced, $p_j \geq \frac{1}{2B}$.
Since $n \geq c_B B\log{\frac{B}{\delta}}$, by the multiplicative Chernoff bound, for some large enough $c_B$, with probability at least $1 - \frac{\delta}{2}$, $|R_j| \geq \frac{p_j}{2}$.

Consider each bin $j$. Let $\mu_j$ be the expected output of $g$ in bin $j$, given by $\mu_j = \expect[g(Z) \; | \; g(Z) \in I_j]$. $\mu(R_j)$, the mean of the values in $R_j$, is the empirical average of $|R_j|$ such values, each bounded between $b_{j-1}$ and $b_j$ where $I_j = [b_{j-1}, b_j]$. So $\hat{\mu}(R_j)$ is sub-Gaussian with parameter:

\[ \sigma^2 = \frac{(b_j - b_{j-1})^2}{4|R_j|} \leq \frac{(b_j - b_{j-1})^2}{2p_jn} \]

Then by the sub-Gaussian tail bound, for any $1 \leq j \leq B$, with probability at least $1 - \frac{\delta}{2B}$, we have:
\[ (\mu_j - \hat{\mu}(R_j))^2 \leq \frac{(b_j - b_{j-1})^2}{p_jn} \log{\frac{4B}{\delta}} \] 

So by union bound with probability at least $1 - \frac{\delta}{2}$ the above holds for all $1 \leq j \leq B$ simultaneously.

We then bound the $\ell_2$-error.
\begin{align*}
||\hat{g_{\mathcal{B}}} - g_{\mathcal{B}}||_2 &= \sqrt{\sum_{j =1}^B p_j (\mu_j - \hat{\mu}(R_j))^2} \\
&\leq \sqrt{\sum_{j =1}^B p_j \frac{(b_j - b_{j-1})^2}{p_jn} \log{\frac{4B}{\delta}}} \\
&\leq \sqrt{\frac{1}{n} \log{\frac{4B}{\delta}} \sum_{j =1}^B (b_j - b_{j-1})^2 } \\
&\leq \sqrt{\frac{1}{n} \log{\frac{4B}{\delta}} \sum_{j =1}^B (b_j - b_{j-1}) } \\
&\leq \sqrt{\frac{1}{n} \log{\frac{4B}{\delta}} } \\
&\leq c_2 \frac{1}{\sqrt{n}} \sqrt{\log{\frac{B}{\delta}}}
\end{align*}

Similarly, we can also bound the $\ell_1$-error. Here we also use the fact that $p_j \leq \frac{2}{B}$ since $\bins{}$ is 2-well-balanced.
\begin{align*}
||\hat{g_{\mathcal{B}}} - g_{\mathcal{B}}||_1 &= \sum_{j =1}^B p_j |\mu_j - \hat{\mu}(R_j)| \\
&\leq \sum_{j =1}^B p_j \sqrt{\frac{(b_j - b_{j-1})^2}{p_jn} \log{\frac{4B}{\delta}}} \\
&\leq \sum_{j =1}^B \sqrt{\frac{p_j(b_j - b_{j-1})^2}{n} \log{\frac{4B}{\delta}}} \\
&\leq \sum_{j =1}^B \sqrt{\frac{2(b_j - b_{j-1})^2}{Bn} \log{\frac{4B}{\delta}}} \\
&\leq \sqrt{\frac{2}{Bn} \log{\frac{4B}{\delta}}} \sum_{j =1}^B (b_j - b_{j-1}) \\
&\leq c_1 \frac{1}{\sqrt{Bn}} \sqrt{\log{\frac{B}{\delta}}}
\end{align*}
By union bound, these hold with probability at least $1 - \delta$, which completes the proof.
\end{proof}

In our proofs we also required that the binning scheme we constructed was 2-well-balanced.

\begin{lemma}[Well-balanced on population]
\label{lem:well-balanced}
If $n \geq \Omega(B \log{\frac{B}{\delta}})$ then the binning scheme $\bins{}$ we construct in step 2 of our algorithm is 2-well-balanced.
\end{lemma}

\begin{proof}
Suppose we are given a bin construction set of size $n$, $T_n = \{(z_1, y_1), \dots, (z_n, y_n)\}$. We want to show that if an interval $I_j$ contains $\frac{n}{B}$ points $g(z_i)$ then $\frac{1}{2B} \leq \prob(g(Z) \in I_j) \leq \frac{2}{B}$. For any interval $I$, let $\hat{P}(I)$ be the empirical estimate of $P(I) = \prob(g(Z) \in I)$ given by:
\[ \hat{P}(I) \]

We begin by describing the challenges and intuition with the proof.
One way to do this is by adapting the standard proof of the concentration of the median.

The idea is that we will cover $[0, 1]$ with 10B small intervals such that for each of these intervals $I_j'$, $P(g(Z) \in I_j') = \frac{1}{10B}$. We will then use Bernstein and union bound to conclude that for each of 

Since $Z$ has density (as defined in Section~\ref{sec:formulation}), and $g$ is injective and continuous, $g(Z)$ has density as well. 
\end{proof}

Finally, we have the tools to prove the main theorem:

\begin{proof}[Proof of Theorem~\ref{thm:final-calib}]
The proof pieces together Lemmas~\ref{lem:platt_scaling_bound},~\ref{thm:empirical-binning},~\ref{lem:well-balanced} and Proposition~\ref{prop:bin_low_bound}.

If $n = \Theta(\frac{(L'd'\log{R})^2}{\epsilon^2})$, from Lemma~\ref{lem:platt_scaling_bound}, step 1 of the variance-reduced calibration algorithm gives us $g$ with $\squaredce(g) \leq c_0^2 \epsilon^2$ for constant $c_0 > 1$.

Next if $n = \Theta(B \log{\frac{B}{\delta}})$, from Lemma~\ref{lem:well-balanced}, step 2 chooses a 2-well-balanced binning scheme $\bins{}$.

From Proposition~\ref{prop:bin_low_bound}, $\ltwoce(g_{\bins{}}) \leq \ltwoce(g) \leq c_0 \epsilon$. Then from Lemma~\ref{thm:empirical-binning}, if $n = \frac{1}{\epsilon^2} \log{\frac{B}{\delta}}$, step 3 gives us $\hat{g_{\bins{}}}$ with $||\hat{g_{\bins{}}} - g_{\bins{}}||_2 \leq c_1 \epsilon$ for constant $c_1 > 0$. We want to say that since $\hat{g_{\bins{}}}$ is close to $g_{\bins{}}$ and $g_{\bins{}}$ has low calibration error, this must mean that $\hat{g_{\bins{}}}$ has low calibration error.

To do this we represent the $\ell_2$ calibration error of any $g$ as the distance between $g$ and a perfectly recalibrated version of $g$. That is, we define the perfectly recalibrated version of $g$ as:
\[ \omega(g)(z) = \expect[ Y \mid g(Z) = z ] \]
Then for any $g$, we can write $\ltwoce(g) = ||g - \omega(g)||_2$. By triangle inequality on the $\ell_2$ norm on functions, we have:
\[ ||\hat{g_{\bins{}}} - \omega(g_{\bins{}})||_2 \leq ||\hat{g_{\bins{}}} - g_{\bins{}}||_2 + ||g_{\bins{}} - \omega(g_{\bins{}})||_2 \leq (c_1 + c_0)\epsilon \]
Now the LHS is not quite the $\ell_2$ calibration error of $\hat{g_{\bins{}}}$, which is $||\hat{g_{\bins{}}} - \omega(\hat{g_{\bins{}}})||_2$~\footnote{This is a very technical point, so at a first pass the reader may skip the following discussion.}.
However, since $g$ is injective, $g_{\bins{}}$ takes on a different value for each interval $I_j \in \bins{}$.
If $\hat{g_{\bins{}}}$ also takes on a different value for each interval $I_j \in \bins{}$, then we can see that $\omega(g_{\bins{}}) = \omega(\hat{g_{\bins{}}})$.
If not, $\omega(\hat{g_{\bins{}}})$ can only merge some of the intervals of $\omega(g_{\bins{}})$, and by Jensen's we can show:
\[ ||\hat{g_{\bins{}}} - \omega(\hat{g_{\bins{}}})||_2 \leq ||\hat{g_{\bins{}}} - \omega(g_{\bins{}})||_2 \leq (c_1 + c_0)\epsilon \]
An alternative way to get this is to add infinitesimal noise to $\hat{g_{\bins{}}}$ for each interval $I_j$, in which case we get $\omega(g_{\bins{}}) = \omega(\hat{g_{\bins{}}})$.
Finally we convert back from the $\ltwoce$ to $\squaredce$:
\[ \squaredce(\hat{g_{\bins{}}}) = ||\hat{g_{\bins{}}} - \omega(\hat{g_{\bins{}}})||_2^2 \leq (c_1 + c_0)^2 \epsilon^2 \]
By e.g. choosing $c_0 = 1.1$ and $c_1 = 0.1$ the LHS becomes $\leq 2\epsilon^2$, which completes the proof.


% From proposition Y, $\twoce(g_{\bins{}}) \leq \ltwoce(g)$
\end{proof}

\subsection{Bounding the mean-squared error}

We also show that if we use lots of bins, discretization has little impact on model quality as measured by the mean-squared error.
Note that recalibration itself typically \emph{reduces/improves} the mean-squared error.
However, in our method after fitting a recalibration function like Platt scaling does, we discretize the function outputs.
This reduces the calibration error and allows us to measure the calibration error, but it does increase the mean-squared error by a small amount.
Here we upper bound the increase in mean-squared error.
In other words, our method allows for the calibration error of the final model to be measured, and has little impact on the mean-squared error.

\begin{restatable}[MSE Bound]{proposition}{mseFiniteBinning}
\label{prop:mse-finite-binning}
If $\mathcal{B}$ is a 2-well-balanced binning scheme of size $B$ and $B = \widetilde{\Omega}(n)$, where $\widetilde{\Omega}$ hides $\log$ factors, then $\mbox{MSE}(\hat{g}_{\mathcal{B}}) \leq \mbox{MSE}(g) + O(\frac{1}{B})$.
\end{restatable}

To show this we begin with a lemma showing that if $f$ and $g$ are close in $\ell_1$ norm, then their mean-squared errors are close:

\begin{lemma}
\label{lem:mse-l1}
For $f, g : \mathcal{Z} \to [0, 1]$, $\mbox{MSE}(f) \leq \mbox{MSE}(g) + 2||f - g||_1$.
\end{lemma}

\begin{proof}
\begin{align*}
\expect[(f(Z) - Y)^2 - (g(Z) - Y)^2] &= \expect[(f(Z) - g(Z))(f(Z) + g(Z) - 2Y)] \\
&\leq \expect[|f(Z) - g(Z)||f(Z) + g(Z) - 2Y|] \\
&\leq \expect[2|f(Z) - g(Z)|] \\
& =2||f-g||_1
\end{align*}
Where the third line followed because $-2 \leq f(Z) + g(Z) - 2Y \leq 2$.
\end{proof}

Next, we show that in the limit of infinite data, if we bin with a well-balanced binning scheme then the MSE cannot increase by much.

\begin{lemma}
\label{thm:bin-sharpness}
Let $\mathcal{B}$ be an $\alpha$-well-balanced binning scheme of size $B$. Then $\mbox{MSE}(g_{\mathcal{B}}) \leq \mbox{MSE}(g) + \frac{2\alpha}{B}$.
\end{lemma}

\begin{proof}
We bound $||g_{\mathcal{B}} - g||_1$ and then use Lemma~\ref{lem:mse-l1}. We use the law of total expectation, conditioning on $\beta(g(Z))$, the bin that $g(Z)$ falls into.
\begin{align*}
||g_{\mathcal{B}} - g||_1 &= \mathbb{E}[|g_{\mathcal{B}}(Z) - g(Z)|] \\
&\leq \mathop{\mathbb{E}}_{\beta(g(Z))} \Big[ \mathop{\mathbb{E}}_{Z | \beta(g(Z))} [ |g_{\mathcal{B}}(Z) - g(Z)| ]\Big]\\
&\leq \mathop{\mathbb{E}}_{\beta(g(Z))} \Big[ b_{\beta(g(Z))} - b_{\beta(g(Z))-1}\Big]
\end{align*}
We now use the fact that $\mathcal{B}$ is $\alpha$-well-balanced.
\begin{align*}
\mathop{\mathbb{E}}_{\beta(g(Z))} \Big[ (b_{\beta(g(Z))} - b_{\beta(g(Z))-1})\Big] &= \sum_{i=1}^B \prob\big(g(Z) \in [b_{\beta(g(Z))-1}, b_{\beta(g(Z))}]\big) (b_{\beta(g(Z))} - b_{\beta(g(Z))-1}) \\
&\leq \sum_{i=1}^B \frac{\alpha}{B} (b_{\beta(g(Z))} - b_{\beta(g(Z))-1}) \\
&\leq \frac{\alpha}{B}
\end{align*}
Finally, from Lemma~\ref{lem:mse-l1}, we get that $\mbox{MSE}(g_{\mathcal{B}}) \leq \mbox{MSE}(g) + \frac{2\alpha}{B}$.
\end{proof}

The above lemma bounds the increase in MSE due to binning in the infinite sample case -- next we deal with the finite sample case and prove proposition~\ref{prop:mse-finite-binning}:

\begin{proof}[Proof of Proposition~\ref{prop:mse-finite-binning}:]
Ignoring all $\log$ factors, from Theorem~\ref{thm:empirical-binning} if $n = \widetilde{\Omega}(B)$, we have $||\hat{g}_{\mathcal{B}} - g_{\mathcal{B}}||_1 = O(\frac{1}{\sqrt{nB}})$. Then from  Lemma~\ref{lem:mse-l1}, $\mbox{MSE}(\hat{g}_{\mathcal{B}}) \leq \mbox{MSE}(g_{\mathcal{B}}) + O(\frac{1}{\sqrt{Bn}}) \leq \mbox{MSE}(g_{\mathcal{B}}) + O(\frac{1}{B})$. From Theorem~\ref{thm:bin-sharpness}, since $\mathcal{B}$ is 2-well-balanced, we have  $\mbox{MSE}(g_{\mathcal{B}}) \leq \mbox{MSE}(g) + O(\frac{1}{B})$. This gives us $\mbox{MSE}(\hat{g}_{\mathcal{B}}) \leq \mbox{MSE}(g) + O(\frac{1}{B})$.
\end{proof}

\subsection{Alternative binning schemes}

We note that there are alternative binning schemes in the literature.
For example, the $B$ bins can be chosen as $I_1 = [0, \frac{1}{B}], I_2 = (\frac{1}{B}, \frac{2}{B}], \dots, I_B = (\frac{B-1}{B}, 1]$.
The main problem with this binning scheme is that we may not be able to measure the calibration error efficiently, which is critical.
However, if we choose the bins like this, and are lucky that the binning scheme happens to be 2-well-balanced, we can improve the bounds on the MSE that we proved above.
This motivates alternative hybrid binning schemes, where we try to keep the width of the bins as close to $1/B$ as possible, while ensuring that each bin contains lots of points as well.
We think analyzing what binning schemes lead to the best bounds, and seeing if this can improve the calibration method, is a good direction for future research.

